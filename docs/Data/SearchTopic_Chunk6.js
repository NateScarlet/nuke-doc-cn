define({"411":{i:0.00113727026462862,u:"../content/comp_environment/3d_compositing/relighting_2d_images.html",a:"The Relight node takes a 2D image containing normal and point position passes and lets you relight it using 3D point lights. Essentially bypassing the need to return to a 3D application and re-render the lighting, Relight provides a quick and interactive way to relight a 3D scene in a 2D ...",t:"Relighting a 2D Image Using 3D Lights"},"412":{i:0.000641994058971331,u:"../content/comp_environment/3d_compositing/cameras.html",a:"Nuke supports multiple cameras in a scene, with each providing a unique perspective.  For details on how to add a camera, look through it, and edit its lens characteristics, see   Working with Cameras . In addition to using cameras to view and render 3D scenes, you can also set up cameras that ...",t:"Cameras"},"413":{i:0.00139657706453725,u:"../content/comp_environment/3d_compositing/working_with_cameras.html",a:"This section explains how to add a camera to your script, look through it, lock it, and edit its lens characteristics. To Add a Camera Click 3D \u003e Camera to insert a Camera node. OR In the 3D Viewer, select create camera to place a new camera at the current position and orientation in 3D space. To ...",t:"Working with Cameras"},"414":{i:0.0007408536817102,u:"../content/comp_environment/3d_compositing/projection_cameras.html",a:"In addition to viewing and rendering a 3D scene, cameras can also project a 2D still image or image sequence onto geometry in the scene. This is similar to the front-projection systems used in practical photography, where a background image or other element is projected onto the stage and ...",t:"Projection Cameras"},"415":{i:0.000412448712061036,u:"../content/comp_environment/3d_compositing/importing_fbx_cameras.html",a:"You can import cameras from FBX files. About FBX Files FBX is a standard 3D file format that gives you access to 3D scenes created in other applications supporting the same format. What you generally have in an .fbx file is an entire 3D scene containing cameras, lights, meshes, non-uniform rational ...",t:"Importing Cameras from FBX Files"},"416":{i:0.000412448712061036,u:"../content/comp_environment/3d_compositing/importing_alembic_cameras.html",a:"You can import cameras  from Alembic files (.abc file format) into a Nuke scene. During the import, Nuke allows you to control which nodes in the Alembic scene get loaded by using an import dialog. If there is only one item in the Alembic file, it loads automatically. For more information on ...",t:"Importing Cameras from Alembic Files"},"417":{i:0.000412448712061036,u:"../content/comp_environment/3d_compositing/importing_cameras_boujou.html",a:"Nuke is shipped with a script called import_boujou.tcl, which lets you load in cameras created with Boujou. To Import a Camera from Boujou: Save the Boujou camera solve as a .txt file. In Nuke, click on a content menu button and select Script Editor. The Script Editor opens in the pane whose content ...",t:"Importing Cameras from Boujou"},"418":{i:0.000321498003524325,u:"../content/comp_environment/3d_compositing/cameras_pointsto3d.html",a:"Animated cameras created in third-party applications, such as Maya, or Nuke\u0027s CameraTracker contain enough parallax information to locate a 3D point in a 2D scene using the PointsTo3D node. PointsTo3D uses three reference frames in an image sequence to extrapolate the location of a 2D point in 3D ...",t:"Locating a 3D Point from an Animated Camera"},"419":{i:0.000458136250325134,u:"../content/comp_environment/3d_compositing/transforming_objects.html",a:"Transform operations include moving, scaling, rotating the objects in your 3D scene. When an object node is active, you can enter specific transform settings in the node parameters (see  Transforming from the Node Properties Panel ), or directly manipulate the object with the transform handles ...",t:"Transforming Geometry, Cameras, and Lights"},"420":{i:0.00296167027835794,u:"../content/comp_environment/3d_compositing/using_transform_handles.html",a:"Transform handles appear when a 3D object with transform capabilities is loaded into the Properties Bin. The colors of the handles correspond to the axes available in 3D space: red transforms the x-axis, green transforms the y-axis, and blue transforms the z-axis. To Move an Object with the ...",t:"Using the Transform Handles"},"421":{i:0.00281535991524896,u:"../content/comp_environment/3d_compositing/transforming_properties_panel.html",a:"The transform handles (see  Using the Transform Handles ) are a convenient way to move objects around in the 3D workspace, but when you want more precision, you should enter values directly into the object’s node panel. The panel also includes transform and rotation order options, which are not ...",t:"Transforming from the Node Properties Panel"},"422":{i:0.000370175584651795,u:"../content/comp_environment/3d_compositing/transformations_pivot_point.html",a:"When you make changes to an object’s position, scaling and rotation, these occur from the location of the object’s origin point or  pivot. By default, the pivot point is located at the intersection of the object’s local axes.  You can offset the pivot point and move it anywhere you like - you can ...",t:"Transformations and the Pivot Point"},"423":{i:0.000910171893723211,u:"../content/comp_environment/3d_compositing/parenting_axis_objects.html",a:"An axis object works as a null object by adding a new transformational axis to which other objects may be parented. Even when objects already have their own internal axes, it’s sometimes useful to parent in a separate axis.  For example, the Axis node has been parented to the other objects in the ...",t:"Parenting to Axis Objects"},"424":{i:0.000700228048827923,u:"../content/comp_environment/3d_compositing/using_transformgeo_node.html",a:"The TransformGeo node allows you to move, rotate, scale, and perform other transformations on several objects merged together with a MergeGeo node. It also lets you connect geometry objects to an Axis node. By doing so, you can move all the connected objects together by using the Axis transformation ...",t:"Using the TransformGeo Node"},"425":{i:0.000370175584651795,u:"../content/comp_environment/3d_compositing/applying_tracks_object.html",a:"Nuke can import channel files and apply the motion data to the transformation parameters of any camera or object. The most common purpose for this is to simulate a practical camera move or move objects along a defined path.  Channel files contain a set of Cartesian coordinates for every frame of ...",t:"Applying Tracks to an Object"},"426":{i:0.000370175584651795,u:"../content/comp_environment/3d_compositing/importing_fbx_transforms.html",a:"FBX is a standard 3D file format that gives you access to 3D scenes created in other applications supporting the same format. What you generally have in an .fbx file is an entire 3D scene containing cameras, lights, meshes, non-uniform rational B-spline (NURBS) curves, transformation, materials, and ...",t:"Importing Transforms from FBX Files"},"427":{i:0.000370175584651795,u:"../content/comp_environment/3d_compositing/importing_alembic_transforms.html",a:"You can import transforms from Alembic files (.abc file format) into a Nuke scene. During the import, Nuke allows you to control which nodes in the Alembic scene get loaded by using an import dialog. If there is only one item in the Alembic file, it loads automatically. For more information on ...",t:"Importing Transforms from Alembic Files"},"428":{i:0.000573674935570926,u:"../content/comp_environment/3d_compositing/adding_motion_blur_3d_scene.html",a:"To create more realism for a 3D scene, you’ll want to add motion blur to it based on the movement of your 3D camera. This can be done in two ways: Adjust the samplesvalue in the ScanlineRender or RayRender node’s Properties panel. The image is sampled multiple times over the shutter period. This way ...",t:"Adding Motion Blur to the 3D Scene"},"429":{i:0.000729592067453732,u:"../content/comp_environment/3d_compositing/adding_motion_blur_scanlinerender.html",a:"To add motion blur using ScanlineRender or RayRender: In the node’s controls, go to the MultiSampletab. Increase the samplesvalue to sample the image multiple times over the shutter time. The higher the value, the smoother the result, but the longer the render time. In the shutter field, enter the ...",t:"Adding Motion Blur Using a Renderer"},"430":{i:0.00484579005735422,u:"../content/comp_environment/3d_compositing/adding_motion_blur_vectorblur.html",a:"Nuke’s VectorBlur node generates motion blur by blurring each pixel into a straight line, using the values from the motion vector channels (u and v channels) to determine the direction of the blur. Compared to generating motion blur using the ScanlineRender node’s MultiSample controls, this is less ...",t:"Adding Motion Blur Using VectorBlur"},"431":{i:0.00358300956997316,u:"../content/comp_environment/3d_compositing/exporting_objects.html",a:" You can export geometry, cameras, light, axes and point clouds into an FBX (.fbx) or Alembic (.abc) file using the WriteGeo node.  Create a WriteGeo node and attach it to a Scene node. In the WriteGeo controls, select fbx or abc in the file type dropdown, and check the objects to export: geometries ...",t:"Exporting Geometry, Cameras, Lights, Axes, or Point Clouds "},"432":{i:0.00178003627940838,u:"../content/comp_environment/3d_compositing/rendering_3d_scene.html",a:"The 3D Viewer displays the scene using an OpenGL hardware render. When you build a scene, Nuke renders high-quality output from the perspective of the camera connected to the render node. The rendered 2D image is then passed along to the next node in the compositing tree, and you can use the result ...",t:"Rendering a 3D Scene"},"433":{i:0.000880622288300524,u:"../content/comp_environment/stereoscopic_films/stereo_projects.html",a:"The title of this chapter is slightly misleading, as Nuke isn’t actually limited to stereoscopic views, but rather provides multi-view support for as many views as you need. The views do not have to be stereo pairs, but since that is the most obvious application, this chapter mainly deals with ...",t:"Stereoscopic Scripts"},"434":{i:0.00130762961021809,u:"../content/comp_environment/stereoscopic_films/setting_up_stereo_views.html",a:"You can  import your footage and let Nuke create the views automatically or set up views in advance in the project settings. This allows you to process the individual views separately or both views together, and see the effect of your changes on each view. If you are likely to need the same views in ...",t:"Setting Up Views for the Script"},"435":{i:0.000962094385847136,u:"../content/comp_environment/stereoscopic_films/loading_multi_view_images.html",a:"Once you have set up the views, you are ready to read your images into Nuke. To make things easier, the images you read in should have the view name or the first letter of the view name in the filename, for example filename.left.0001.exr, filename.l.exr, or lefteyefilename.0001.cin. If you are using ...",t:"Loading Multi-View Images"},"436":{i:0.000836513913366488,u:"../content/comp_environment/stereoscopic_films/displaying_views.html",a:"You can only display the views that exist in your project settings. To see a list of these views, or to add or delete views, select Edit \u003e Project Settings and go to the Views tab. For more information, see  Setting Up Views for the Script . To Display a Particular View Add a Viewer into your script ...",t:"Displaying Views in the Viewer"},"437":{i:0.000396351981200522,u:"../content/comp_environment/stereoscopic_films/selecting_views_to_change.html",a:"By default, Nuke applies any changes you make to all views of the processed node. To apply changes to a particular view only (for example, the left view but not the right), you must first do one of the following: In the case of most nodes, split the view off in the node’s controls.  In the case of ...",t:"Selecting Which Views to Apply Changes To"},"438":{i:0.000816788165978267,u:"../content/comp_environment/stereoscopic_films/performing_different_actions.html",a:"In case you need to perform totally different actions on the two views, you can add a OneView node to separate one view for processing. To Extract a View for Processing Select Views \u003e OneView to insert a  OneView node in an appropriate place in your script. In the OneView node’s controls, select the ...",t:"Performing Different Actions on Different Views"},"439":{i:0.000396351981200522,u:"../content/comp_environment/stereoscopic_films/reproducing_changes.html",a:"When rotoscoping, creating paint effects, or doing other operations dependent on image locality, you can have changes made to one view automatically reproduced in the other. This applies to the RotoPaint node, Roto node and any nodes, groups, or gizmos that have controls for x and y coordinates. To ...",t:"Reproducing Changes Made to One View"},"440":{i:0.000669628474802141,u:"../content/comp_environment/stereoscopic_films/swapping_views.html",a:"You can rearrange the views in your script using the ShuffleViews node. For example, you can swap the left and right views around in the pipeline, so that Nuke uses the left input for the right eye and vice versa. To Rearrange Views Select Views \u003e ShuffleViews to insert a ShuffleViews node in an ...",t:"Swapping Views"},"441":{i:0.000631516549146798,u:"../content/comp_environment/stereoscopic_films/converting_images_anaglyph.html",a:"You can use the Anaglyph node to convert your inputs into anaglyph images, which produce a 3D effect when viewed with 2-color anaglyph glasses. To Convert Your Images into Anaglyph Select Views \u003e Stereo \u003e Anaglyph to insert an Anaglyph node in an appropriate place in your script. Use the views ...",t:"Converting Images into Anaglyph"},"442":{i:0.000464671104600927,u:"../content/comp_environment/stereoscopic_films/changing_convergence.html",a:"The ReConverge node lets you shift convergence (the inward rotation of the eyes or cameras) so that any selected point in the image appears at screen depth when viewed with 3D glasses. This point is called the convergence point. It is the point where the lines of sight from the two cameras meet. At ...",t:"Changing Convergence"},"443":{i:0.000396351981200522,u:"../content/comp_environment/stereoscopic_films/previewing_stereo_images.html",a:"You can preview stereo images using the Viewer stereo modes or by flipbooking the sequence using Nuke\u0027s default flipbook. Previewing Using the Viewer Stereo Modes The Viewer stereo modes allow you to see both views at once, in either anaglyph or interlaced per scanline. To enable of Viewer stereo ...",t:"Previewing Stereoscopic Images"},"444":{i:0.000321498003524325,u:"../content/comp_environment/stereoscopic_films/rendering_stereo_images.html",a:"You can render several views using a single Write node. When using the stereo extensions for the .exr file format, Nuke writes the output of both views into a single file. With any other file types, the views are written into their respective files. Rendering .exr Files To render .exr files: Select ...",t:"Rendering Stereoscopic Images"},"445":{i:0.000647699535595853,u:"../content/comp_environment/deep/deep_compositing.html",a:"Deep compositing is a way of compositing digital images using data in a different format to standard \"flat\" compositing. As the name suggests, deep compositing uses additional depth data. This reduces need for re-rendering, produces high image quality, and helps you solve problems with artifacts ...",t:"Deep Compositing"},"446":{i:0.00111828241943277,u:"../content/comp_environment/deep/reading_deep_footage.html",a:"You read in deep images to Nuke with a DeepRead node, which is rather like reading in any other images with the Read node. Nuke allows you to import deep images in two formats:  DTEX (generated from Pixar’s PhotoRealistic RenderMan® Pro Server). Scanline OpenEXR 2.2, or above (tiled OpenEXR  files ...",t:"Reading in Deep Footage"},"447":{i:0.00169115574383546,u:"../content/comp_environment/deep/creating_deep_data.html",a:"You can create deep data in Nuke by: sampling a regular 2D image sequence at multiple frames to create several samples for each pixel in a single deep frame. See  Converting a 2D Image Sequence to a Deep Frame Using Input Frames . converting a regular 2D image to a deep image with a single sample ...",t:"Creating Deep Data"},"448":{i:0.000560835129789987,u:"../content/comp_environment/deep/merging_deep_images.html",a:"Use the DeepMerge node to merge the samples from multiple deep images, so that each output pixel contains all the samples from the same pixel of each input. Connect the data you want to merge to the DeepMerge node’s numbered inputs.  In the DeepMerge properties, make sure operation is set to ...",t:"Merging Deep Images"},"449":{i:0.000760195215023472,u:"../content/comp_environment/deep/creating_holdouts.html",a:"There are two ways to create holdouts, using either the DeepHoldout or the DeepMerge nodes. The primary difference is whether you want the node to produce a flattened output image or a deep output image after the holdout.  Creating a Flattened Holdout with the DeepHoldout Node The DeepHoldout node ...",t:"Creating Holdouts"},"450":{i:0.00146336425905134,u:"../content/comp_environment/deep/creating_2d_3d_elements.html",a:"You can create a 2D image or a 3D point cloud from a deep image. Creating a 2D Image from a Deep Image You can use the DeepToImage node to flatten an image, in other words merge all the samples in a deep image into a regular 2D image.  Connect the node to a deep image (or a DeepMerge with merged ...",t:"Creating 2D and 3D Elements from Deep Images"},"451":{i:0.000868050990727563,u:"../content/comp_environment/deep/modifying_deep_data.html",a:"Nuke allows you to color correct deep images as well as modify them using expressions. Color Correcting Deep Images The DeepColorCorrect node applies the color correction to each sample at each pixel.  There are control sets for adjusting shadows, midtones and highlights, as well as a master set for ...",t:"Modifying Deep Data"},"452":{i:0.000959809698883227,u:"../content/comp_environment/deep/cropping_reformatting_deep.html",a:"You can crop, reformat and transform deep images much in the same way as you would a regular image, using the corresponding deep nodes. Remember that since the samples at each pixel can be located at arbitrary depths, resampling during transforming may produce unexpected results since there may not ...",t:"Cropping, Reformatting, and Transforming Deep Images"},"453":{i:0.000686533205281608,u:"../content/comp_environment/deep/sampling_deep_images.html",a:"You can use the DeepSample node to sample any given pixel in a deep image. The Deep Sample node gives you the depth data as figures.  Connect the DeepSample node to another Deep node.  Position the pos indicator over the pixels you want to sample in the Viewer.  View the deep sample information in ...",t:"Sampling Deep Images"},"454":{i:0.00106605667068319,u:"../content/comp_environment/deep/writing_deep_data.html",a:"You can write out deep images in the scanline OpenEXR 2.2, or above, format using the DeepWrite node, which shares a lot of controls with the standard Write node. Do the following: Select Deep \u003e DeepWrite to insert a DeepWrite node into your script. In the properties panel, click the file or proxy ...",t:"Writing Deep Data"},"455":{i:0.00167904675030632,u:"../content/comp_environment/metadata/working_file_metadata.html",a:"Working with File Metadata The Read node\u0027s Metadata tab and the nodes in the Metadata menu of the Toolbar let you work with information embedded in your images. This section gives instructions on their usage, teaching you to view, compare, edit, and render metadata. ",t:"Working with File Metadata"},"456":{i:0.000321498003524325,u:"../content/comp_environment/metadata/metadata_nuke.html",a:"Metadata is a set of information about an image embedded in the image file. This information may include the image’s original bit depth, width, and height, for example. It can be attached to the file by the camera used to shoot the images, and/or edited later.  When Nuke loads an image, it reads in ...",t:"Metadata"},"457":{i:0.000649429795846268,u:"../content/comp_environment/metadata/viewing_metadata.html",a:"The simplest way to view file metadata is by clicking the Metadata tab in the Properties panel  of a standard Read node. All the available metadata is displayed, along with a simple search function. To filter the lists of metadata, use the search metadata for field. For example, if you enter f in ...",t:"Viewing Metadata"},"458":{i:0.000649429795846268,u:"../content/comp_environment/metadata/comparing_metadata.html",a:"To compare metadata between two inputs: From the Toolbar, select MetaData \u003e CompareMetaData to add a CompareMetaData node after the two nodes whose metadata you want to compare. Connect the nodes you want to compare to the A and B inputs of the CompareMetaData node. A list of keys where there is a ...",t:"Comparing Metadata Between Inputs"},"459":{i:0.000649429795846268,u:"../content/comp_environment/metadata/modifying_metadata.html",a:"There are several ways to modify metadata in Nuke. To Add Metadata Select MetaData \u003e ModifyMetaData to insert a ModifyMetaData node after the node whose metadata you want to add a new key to. In the ModifyMetaData controls, click on the plus (+) button. A placeholder appears in the metadata box. ...",t:"Modifying Metadata"},"460":{i:0.000649429795846268,u:"../content/comp_environment/metadata/copying_filtering_metadata.html",a:"Copying Metadata from One Input to Another  and Filtering Metadata To copy metadata from one input to another and/or filter metadata: Select MetaData \u003e CopyMetaData to insert a CopyMetaData node into your script. Connect: the Image input to the node whose image you want to pass down the tree. the ...",t:"Copying Metadata from One Input to Another and Filtering Metadata"},"461":{i:0.000649429795846268,u:"../content/comp_environment/metadata/adding_timecode_metadata.html",a:"To add a time code to metadata: Select MetaData \u003e AddTimeCode to insert an AddTimeCode node into your node tree. A time code is added to the metadata being passed through. By default, the time code is 01:00:00:00 on the first frame. It is updated throughout the frame range according to the input ...",t:"Adding a Time Code to Metadata"},"462":{i:0.000321498003524325,u:"../content/comp_environment/metadata/rendering_metadata.html",a:"When rendering with the Write node, Nuke lets you write out metadata into the following file formats: .exr, .cin, .dpx, and .jpg. You cannot write out metadata into any other formats. When rendering metadata into an .exr file, you can use the metadata dropdown menu in the Write node controls to ...",t:"Rendering Metadata"},"463":{i:0.00142554293673466,u:"../content/comp_environment/metadata/accessing_metadata_tcl.html",a:"You can access metadata via Tcl expressions in the following ways: To get a list of all keys in the incoming metadata, use the expression [metadata]. For example, if you add a Text node after an image and enter [metadata] in the message field, a list of all the keys in the incoming metadata appears ...",t:"Accessing Metadata Using Tcl Expressions"},"464":{i:0.000321498003524325,u:"../content/comp_environment/metadata/accessing_metadata_python.html",a:"Accessing Metadata Using Python You can also access metadata using the Python programming language. For more information, see the Nuke Python documentation (Help \u003e Documentation).",t:"Accessing Metadata Using Python"},"465":{i:0.0020299514405313,u:"../content/comp_environment/audio_in_nuke/audio_nuke.html",a:"In many compositing projects it’s vital to be able to key visual changes to cues on the audio track that goes with the picture. You can use Nuke’s AudioRead node to read in an audio file, view it in the Curve Editor and Dope Sheet in order to line up keyframes of your composition with the waveform ...",t:"Audio in Nuke"},"466":{i:0.00165883761089518,u:"../content/comp_environment/audio_in_nuke/reading_audio_files.html",a:"You can drag and drop audio clips from the Project tab to the Node Graph, if the clip is already in Nuke.  Otherwise, use the AudioRead node to read in an audio file: To create an AudioRead node, click Other \u003e AudioRead in the Nuke Toolbar.  The AudioRead node doesn’t have to be connected to other ...",t:"Reading Audio Files into the Node Graph"},"467":{i:0.000896661403720693,u:"../content/comp_environment/audio_in_nuke/creating_audio_curves.html",a:"Once you have read in an audio file (see  Reading Audio Files into the Node Graph ), you can display an audio waveform for your audio clip and access its animation curve in the Curve Editor or the Dope Sheet. Creating a Keyframe Curve  In the curves section of the AudioRead properties panel, you can ...",t:"Creating and Editing Audio Curves"},"468":{i:0.000896661403720693,u:"../content/comp_environment/audio_in_nuke/flipbooking_audio_track.html",a:"Flipbooking the Audio Track When you’re done, you can proceed to flipbooking your results:  Click the flipbook this Viewer button   in the Comp Viewer. In the Flipbook dialog, select the AudioRead file you want to use in the Audio dropdown.  Click OK. View and listen to your clip in Flipbook. ",t:"Flipbooking the Audio Track"},"469":{i:0.000627405540877939,u:"../content/comp_environment/rendering/previews_rendering.html",a:"Nuke supports a fast, high-quality internal renderer, with superior color resolution and dynamic range without a slowdown in the workflow.  About Rendering in Nuke These are some of the key features of Nuke’s rendering engine:  Multi-threaded rendering to take advantage of multiple processors in its ...",t:"Previews and Rendering"},"470":{i:0.000428158630868161,u:"../content/comp_environment/rendering/previewing_output.html",a:"This section explains how to preview individual frames in a Nuke Viewer window (see  Previewing in a Nuke Viewer ), how to render a  flipbook for a sequence of frames (see  Flipbooking Sequences ), and how to preview output on an external broadcast video monitor (see  Previewing on an External ...",t:"Previewing Output"},"471":{i:0.000442811258677388,u:"../content/comp_environment/rendering/previewing_nuke_viewer.html",a:"When you connect a Viewer to a given node’s output (by selecting the node and pressing a number key), Nuke immediately starts rendering the output in the Viewer using all available local processors. Keep in mind the following tips in order to speed up this type of preview rendering: First, if you ...",t:"Previewing in a Nuke Viewer"},"472":{i:0.000886375378008574,u:"../content/comp_environment/rendering/flipbooking_sequences.html",a:"Flipbooking a sequence refers to rendering a range of images (typically at  proxy resolution), then playing them back in order to accurately access the motion characteristics of added effects. You have a few options for flipbooking within Nuke: You can enable automatic disk caching of rendered ...",t:"Flipbooking Sequences"},"473":{i:0.000868417562938005,u:"../content/comp_environment/rendering/previewing_external_monitor.html",a:"To check the final result in correct video colorspace and pixel aspect ratio, you can preview the current Viewer image on an external broadcast video monitor. This option requires additional hardware, such as a monitor output card or a FireWire port. Our monitor out architecture interfaces directly ...",t:"Previewing on an External Broadcast Video Monitor"},"474":{i:0.000648072447748431,u:"../content/comp_environment/rendering/rendering_output.html",a:"Nuke can render images locally on your workstation (see  Output (Write) Nodes ) or it can be setup to render images on a network render farm (see  Using the Frame Server on External Machines ). Before rendering, make sure that you are using the appropriate file name syntax (see  File Name ...",t:"Rendering Output"},"475":{i:0.000413309723761146,u:"../content/comp_environment/rendering/render_resolution_format.html",a:"Before rendering a script, it’s important to check what is the currently active mode: the full-size or the proxy mode. Nuke executes all renders at the currently active scale. Thus, when a script is rendered in proxy mode, processing is done at the proxy scale and image output goes to the file name ...",t:"Render Resolution and Format"},"476":{i:0.00312209369007532,u:"../content/comp_environment/rendering/output_write_nodes.html",a:"With the correct resolution and format selected, you then insert Write nodes to indicate where you want to render images from the script. One Write node is usually placed at the bottom of the compositing tree to render the final output. However, Write nodes have both input and output connectors, so ...",t:"Output (Write) Nodes"},"477":{i:0.000413309723761146,u:"../content/comp_environment/rendering/file_name_conventions.html",a:"There is no parameter in the Write node to specify output format. Instead, format is indicated by a prefix or an extension when you type the file name. Here is the appropriate syntax: \u003cprefix\u003e:/\u003cpath\u003e/\u003cname\u003e.\u003cframe number variable\u003e.\u003cextension\u003e The optional \u003cprefix\u003e: can be any valid extension. ...",t:"File Name Conventions for Rendered Images"},});