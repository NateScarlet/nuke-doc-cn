define({"682":{i:0.000460366040570847,u:"../content/comp_environment/cameratracker/transforming_scene.html",a:"In addition to defining the ground plane, axes, and origin automatically, you can manually adjust the scene using the Scene tab\u0027s Scene Transform controls. These controls operate in exactly the same way as other transform tools in Nuke. To transform a scene, use the following controls: rotation ...",t:"Transforming the Scene Manually"},"683":{i:0.000851682304490853,u:"../content/comp_environment/cameratracker/creating_contraints.html",a:"Scene constraints enable you to use known distances, measured on set, to scale a scene more accurately. They are applicable to both Auto Tracks and User Tracks. The unit of measure used is up to you, because Nuke equates any value as being equal to one unit in 3D space. The image shows a point cloud ...",t:"Using Scene Constraints"},"684":{i:0.000399023685562183,u:"../content/comp_environment/cameratracker/using_solve_data.html",a:"Using Solve Data Once your camera solve is correctly aligned, you can create animated or baked cameras, multi-view rigs, 3D scenes and point cloud, lens distortion, or cards from the camera data. See: Creating Camera Nodes . Creating Scenes . Creating Point Clouds . Creating Cards .",t:"Using Solve Data"},"685":{i:0.000674276491857359,u:"../content/comp_environment/cameratracker/creating_cameras.html",a:"Using the tracking and solve data, CameraTracker can create linked or baked cameras that emulate the original camera track on set. You can create a single camera or a camera rig for multi-view projects, depending on your script. Creating a Camera Node Select Camera from the Export dropdown menu. ...",t:"Creating Camera Nodes"},"686":{i:0.000406291672072859,u:"../content/comp_environment/cameratracker/creating_scenes.html",a:"CameraTracker can create a ready-to-use 3D scene containing a point cloud, camera, and Scene node from the track and solve data. The Scene+ option adds LensDistortion and ScanlineRender nodes in addition to the standard scene nodes.  Creating a 3D Scene Select Scene from the Export dropdown menu. ...",t:"Creating Scenes"},"687":{i:0.000406291672072859,u:"../content/comp_environment/cameratracker/creating_pointclouds.html",a:"CameraTracker can create a baked point cloud, that is, points that don\u0027t update when you make changes to the CameraTracker properties panel controls.  Select Point cloud from the Export dropdown menu. Click Create. CameraTracker adds a CameraTrackerPointCloud node to the Node Graph. Double-click the ...",t:"Creating Point Clouds"},"688":{i:0.000406291672072859,u:"../content/comp_environment/cameratracker/creating_cards.html",a:"CameraTracker can automatically create a 3D card for each solved frame using the camera to project the image at that frame onto the card. Select Cards from the Export dropdown menu. Enable or disable the Link output control to determine whether the cameras are expression linked or baked: When ...",t:"Creating Cards"},"689":{i:0.000729053730614065,u:"../content/comp_environment/cameratracker/combining_solves.html",a:"Sometimes, you might have multiple footage sources of the same scene or content available. For example, you might have footage from  witness cameras, or someone has taken detailed still photos of the scene. CameraTracker provides a way to solve each of your sources and then register them all in the ...",t:"Combining Solves"},"690":{i:0.000399023685562183,u:"../content/comp_environment/cameratracker/placing_objects_scene.html",a:"You can use the camera and point cloud to add geometry to the scene. You can add objects manually,  but placing them inside the camera’s field of vision at the desired position can be time consuming. CameraTracker provides an automatic creation function to help you achieve the results you need. In ...",t:"Placing Objects in the Scene"},"691":{i:0.000693962391385327,u:"../content/comp_environment/cameratracker/lens_distortion.html",a:"By default, any CG elements you add to your 3D scene do not have lens distortion applied to them. As a result, when you combine them with your 2D footage, they can look like they weren\u0027t shot with the same camera. To fix this, you can: Use CameraTracker to calculate the lens distortion on your 2D ...",t:"Accounting for Lens Distortion"},"692":{i:0.000321498003524325,u:"../content/comp_environment/cameratracker/lens_type.html",a:"Normally, you do not need to set the Lens parameters on the Output tab, as CameraTracker does this for you, but it may be helpful to know a little about the equations CameraTracker uses to account for lens distortion. There are two modes, depending on the lens type detected: Spherical - compensates ...",t:"About the Lens Model"},"693":{i:0.000571034420468054,u:"../content/comp_environment/matchgrade/using_matchgrade.html",a:"This chapter teaches you how to use MatchGrade to automatically calculate a grade to match the colors in the Source input to the colors in the Target input. Introduction  You can use MatchGrade to: extract a baked-in grade if the Target clip that you want to match  is simply a graded version of the ...",t:"Using MatchGrade"},"694":{i:0.00375982690083993,u:"../content/comp_environment/matchgrade/extracting_baked_grade.html",a:"Select Color \u003e MatchGrade to insert a MatchGrade node into your script. Do the following: Connect MatchGrade\u0027s Source input to the clip to which you want to apply the color transform. Connect the Target input to the clip you want to match. This should be a graded version of the Source. Connect a ...",t:"Extracting a Baked-In Grade"},"695":{i:0.000564191170111493,u:"../content/comp_environment/matchgrade/matching_different_clips.html",a:"Select Color \u003e MatchGrade to insert a MatchGrade node into your script. Do the following: Connect MatchGrade\u0027s Source input to the clip to which you want to apply the color transform. Connect the Target input to the clip you want to match. This should be a completely different clip, a stereo pair ...",t:"Matching a Grade Between Two Different Clips"},"696":{i:0.000376324095958171,u:"../content/comp_environment/smartpaint/smartvector.html",a:"The Smart Vector toolset allows you to work on one frame in a sequence and then use motion vector information to accurately propagate work throughout the rest of the sequence. The vectors are generated in the SmartVector node and then piped into the VectorDistort node to warp the work, that way, you ...",t:"Using the Smart Vector Toolset"},"697":{i:0.0013514132472373,u:"../content/comp_environment/smartpaint/generating_vectors.html",a:"The Smart Vector toolset generates motion vectors once, up front, to reduce overheads later on. The SmartVector node writes motion vectors to the .exr format, which are then used to drive the VectorDistort node. To generate motion vectors: Read in your source sequence and then connect the ...",t:"Generating Motion Vectors"},"698":{i:0.00234696915046004,u:"../content/comp_environment/smartpaint/adding_paint.html",a:"The Smart Vector toolset minimizes the paint work needed to create your final comp by propagating paint through a sequence from a single source frame. To add paint to your sequence: Scrub through your sequence to find a good reference frame. Good reference frames should: be at the point of least ...",t:"Adding Paint to the Source"},"699":{i:0.000975796948646972,u:"../content/comp_environment/smartpaint/adding_images.html",a:"The Smart Vector toolset can also propagate an image through a sequence from a single source frame. To add an image to your sequence: Scrub through your sequence to find a good reference frame. Good reference frames should: be at the point of least motion, also known as the motion apex, contain ...",t:"Adding an Image to the Source"},"700":{i:0.00773951174410498,u:"../content/comp_environment/smartpaint/generating_distortion.html",a:"Once you\u0027ve generated your motion vectors and added your paint to the source sequence, the VectorDistort node takes the paint from the reference frame and propagates it through the rest of the sequence using the motion vectors from the SmartVector node. Warping the Vectors After generating your ...",t:"Applying Motion Vectors to the Source"},"701":{i:0.00251421228846393,u:"../content/comp_environment/smartpaint/improving_warps.html",a:"For short sequences, or sequences with minimal movement and detail, your corrections should propagate nicely. If you see that your corrections warp incorrectly over time, you can use VectorDistort\u0027s frame distance control to massage the results. Warping relies on good vectors from SmartVector and a ...",t:"Improving Warps"},"702":{i:0.00322652282760121,u:"../content/comp_environment/smartpaint/multi_reference_frames.html",a:"Some sequences involve movement and detail that can\u0027t be propagated from a single frame correction. In these cases, you can use multiple VectorDistort nodes with different reference frames to minimize the amount of correction work needed. Set up your first paint correction as described in  Adding ...",t:"Warping Multiple Reference Frames"},"703":{i:0.00155789108795043,u:"../content/comp_environment/depthgenerator/generating_depth_maps.html",a:"You can use the  DepthGenerator node in NukeX to generate a depth map from your footage. The node uses information from a tracked camera to create a channel that displays variations in depth. About Depth Maps A depth map is an image that uses the brightness of each pixel to specify the distance ...",t:"Generating Depth Maps"},"704":{i:0.000586344663537045,u:"../content/comp_environment/depthgenerator/connecting_depthgenerator.html",a:"To connect DepthGenerator: To use the DepthGenerator node, you need a tracked camera that matches your footage. If you don’t already have one, you can create one with the CameraTracker node (see  Camera Tracking ). Create a DepthGenerator node by clicking 3D \u003e DepthGenerator. Attach a Viewer to the ...",t:"Connecting DepthGenerator"},"705":{i:0.000835545409908807,u:"../content/comp_environment/depthgenerator/selecting_output.html",a:"To select what to output: In the DepthGenerator properties, use Depth Output to select the type of depth map you want to generate: Depth (1/Z) - output 1/Z where Z is the distance along the Z axis for the camera. This matches the depth output of the ScanlineRender node. This mode also allows you to ...",t:"Selecting What to Output"},"706":{i:0.00129657118949281,u:"../content/comp_environment/depthgenerator/analyzing_depth.html",a:"To analyze the depth: Do one of the following: Use the Frame Separation control to select the offset between the current frame and the frame against which to calculate depth for your footage. For example, if your current frame is 100, and your frame separation is 2, DepthGenerator uses frame 98 and ...",t:"Analyzing the Depth"},"707":{i:0.00168845203839003,u:"../content/comp_environment/depthgenerator/refining_results.html",a:"To refine the results: Adjust Depth Detail to vary the resolution of the images used to calculate the depth map. The default value of 0.5 equals half the image resolution. Lower values speed up processing and deliver a smoother result. Higher values pick up finer details, but also increase ...",t:"Refining the Results"},"708":{i:0.00202155997787838,u:"../content/comp_environment/depthgenerator/using_results.html",a:"Once you are happy with the results from DepthGenerator, there are several ways you can use them in Nuke and NukeX: If you set Depth Output to Depth 1/Z,you can click Create Card in the DepthGenerator controls to create a Group node with a Card positioned in 3D space and displaced according to the ...",t:"Using the Results"},"709":{i:0.0011329746750443,u:"../content/comp_environment/point_clouds_meshes/creating_point_clouds_meshes.html",a:"Dense point clouds are a useful starting point for 3D modeling and can be helpful in positioning 3D objects into a scene. Using the PointCloudGenerator node, you can create a dense point cloud based on the information generated by CameraTracker and use the points to create a 3D mesh of your 2D ...",t:"Creating Dense Point Clouds"},"710":{i:0.000459075663084716,u:"../content/comp_environment/point_clouds_meshes/connecting_pointcloudgenerator.html",a:"To create a dense point cloud, PointCloudGenerator needs a Camera node with a solved camera path, either from CameraTracker or from a third party 3D application, and the tracked 2D footage. To connect the node, do the following: Click 3D \u003e Geometry \u003e PointCloudGenerator. Connect the necessary nodes ...",t:"Connecting the PointCloudGenerator Node"},"711":{i:0.000516607871441726,u:"../content/comp_environment/point_clouds_meshes/masking_regions_image.html",a:"If you don’t want to track all regions of your image, for example, if there are moving objects or reflections in the image, you can attach a matte to the Mask input to define image regions that should not be tracked. You can also use the source input’s alpha channel as a matte. If you want to use a ...",t:"Masking Out Regions of the Image"},"712":{i:0.000654185531002117,u:"../content/comp_environment/point_clouds_meshes/setting_keyframes_sequence.html",a:"Keyframes represent the best frames in a sequence to perform a particular function, in this case tracking. PointCloudGenerator can analyze the sequence for you, automatically picking frames with good parallax, or you can set keyframes manually by marking the frames yourself. Setting Keyframes ...",t:"Setting Keyframes in a Sequence"},"713":{i:0.00152939432542793,u:"../content/comp_environment/point_clouds_meshes/tracking_dense_point_cloud.html",a:"The next step towards creating a dense point cloud is to track your footage for more 3D feature points using the information from keyframes in the sequence and the solved Camera. Click Track Points to track the sequence and create a dense point cloud. The default settings work well on most ...",t:"Tracking a Dense Point Cloud"},"714":{i:0.00175908290011555,u:"../content/comp_environment/point_clouds_meshes/filtering_your_point_cloud.html",a:"You can filter your point cloud by adjusting the number and quality of points it includes. NukeX filters the tracked point cloud directly, without recalculating, streamlining your workflow. Ensure that Display rejected points is enabled and adjust the threshold controls to update the 3D Viewer ...",t:"Filtering Your Point Cloud"},"715":{i:0.00106912232573805,u:"../content/comp_environment/point_clouds_meshes/removing_rejected_points.html",a:"Once you’ve identified and highlighted the less accurate points: Click Delete Rejected Points to automatically remove all highlighted points. You can then be a little more selective by switching the Viewer to Vertex selection mode. Use the Output controls to assist you when visualizing your point ...",t:"Removing Rejected Points"},"716":{i:0.00136784736528994,u:"../content/comp_environment/point_clouds_meshes/grouping_labeling_baking_points.html",a:"Point clouds can be awkward to interpret, especially if the Point Separation control is set to a relatively high value. Grouping points in the cloud can help to visualize the scene, particularly when sensibly labeled and colored in the Viewer. Groups can also be baked out as separate point clouds or ...",t:"Grouping, Labeling, and Baking Points"},"717":{i:0.00162176963111114,u:"../content/comp_environment/point_clouds_meshes/creating_mesh_point_cloud.html",a:"PointCloudGenerator can create meshes from grouped points in the point cloud that you can use as stand-alone 3D objects, for example, in 3D modeling. You can also use these meshes to quickly project the 2D sequence onto the mesh using the Project3D node. PointCloudGenerator meshes are based on the ...",t:"Creating a Mesh from a Point Cloud"},"718":{i:0.00170003206879343,u:"../content/comp_environment/point_clouds_meshes/adding_texture_mesh.html",a:"You can quickly add texture to your point cloud mesh using the Project3D node. Do the following: Click 3D \u003e Shader \u003e Project3D. Connect the solved Camera node to the Project3D cam input. Connect the other input of the Project3D node into the sequence. Connect the Group1_Mesh node img input to the ...",t:"Adding Texture to a Mesh"},"719":{i:0.000604993920052436,u:"../content/comp_environment/point_clouds_meshes/using_poissonmesh.html",a:"The PoissonMesh node uses information from a dense point cloud to generate a mesh that you can further use as a 3D object, in 3D modeling for instance. The PoissonMesh node is based on the Poisson Surface Reconstruction calculation method. The original source code and paper were created by Michael ...",t:"Using the PoissonMesh Node"},"720":{i:0.000321498003524325,u:"../content/comp_environment/point_clouds_meshes/adding_texture_poissonmesh.html",a:"You can add texture to your point cloud mesh using the Project3D and ApplyMaterial nodes. Do the following:  Connect your texture node to the Project3D node. Then connect the Project3D node to the mat input of the ApplyMaterial. Connect the other input of the ApplyMaterial node into the PoissonMesh ...",t:"Adding Texture to the PoissonMesh"},"721":{i:0.000742931948439202,u:"../content/comp_environment/modelbuilder/creating_3d_models.html",a:"The ModelBuilder node provides an easy way to create 3D models for 2D shots. You can build a model by creating shapes and then editing them, and align models over your 2D footage by dragging vertices to their corresponding 2D location. Prerequisites To be able to align models, ModelBuilder needs a ...",t:"Using ModelBuilder"},"722":{i:0.000411712592762673,u:"../content/comp_environment/modelbuilder/connecting_modelbuilder.html",a:"To connect the ModelBuilder node: If you don’t already have a tracked camera that matches your footage, create one using the CameraTracker node (see  Camera Tracking ). ModelBuilder needs a changing Camera (with a change of more than 5 degrees) in order to calculate 3D points from your 2D footage. ...",t:"Connecting the ModelBuilder Node"},"723":{i:0.000528366047260182,u:"../content/comp_environment/modelbuilder/creating_shapes.html",a:"To create shapes: Double-click on the ModelBuilder node to open its properties. Use the shape creation menu   in the ModelBuilder toolbar on the left hand side of the Viewer to select the basic 3D shape that best matches the object you are trying to model. You can select between Point, Card, Cube, ...",t:"Creating Shapes"},"724":{i:0.000860829994818338,u:"../content/comp_environment/modelbuilder/shapes_display_characteristics.html",a:"To edit shapes\u0027 display characteristics: If you created a lot of shapes, you may want to organize them into groups. Click the + button in the ModelBuilder properties to create a Group item and drag shapes inside it to move them into the group. To rename shapes or groups, click on them in the Scene ...",t:"Editing Shapes’ Display Characteristics"},"725":{i:0.000655618131025566,u:"../content/comp_environment/modelbuilder/positioning_shapes.html",a:"To position shapes: If the Align Mode isn’t already active, click    in the ModelBuilder toolbar to activate it. The Viewer should also automatically be locked to your input Camera node. If this isn’t the case, select the Camera from the dropdown menu in the top right corner of the Viewer and click  ...",t:"Positioning Shapes"},"726":{i:0.000968996487190733,u:"../content/comp_environment/modelbuilder/editing_shapes.html",a:"When editing ModelBuilder shapes, there are two types of editing actions: Single-step actions that happen instantly, such as extruding and merging faces. Multi-step actions (such as beveling and subdiving faces) that have extra parameters you can set. All multi-step actions behave in the same way: ...",t:"Editing Shapes"},"727":{i:0.000486230088451903,u:"../content/comp_environment/modelbuilder/editing_vertices.html",a:"To edit vertices: Activate Edit Mode by selecting   from the ModelBuilder toolbar. Set the selection mode menu to Select vertices  in the ModelBuilder toolbar and select one or more vertices on the object. Edit your selection as necessary: To translate, rotate, or scale the selected vertices, drag ...",t:"Editing Vertices"},"728":{i:0.000486230088451903,u:"../content/comp_environment/modelbuilder/editing_edges.html",a:"To edit edges and edge loops: Activate Edit Mode by selecting   from the ModelBuilder toolbar. Set the selection mode menu to either Select edges  or Select edge loops  in the ModelBuilder toolbar and select one or more edges or edge loops on the object. An edge loop is a string of connected edges ...",t:"Editing Edges and Edge Loops"},"729":{i:0.000486230088451903,u:"../content/comp_environment/modelbuilder/editing_faces.html",a:"To edit faces: Activate Edit Mode by selecting   from the ModelBuilder toolbar. Set the selection mode menu to Select faces    in the ModelBuilder toolbar and select one or more faces on the object. Edit your selection as necessary: To translate, rotate, or scale the selected faces, drag the ...",t:"Editing Faces"},"730":{i:0.000486230088451903,u:"../content/comp_environment/modelbuilder/editing_objects.html",a:"To edit objects: Activate Edit Mode by selecting   from the ModelBuilder toolbar. Click the Select object button   in the ModelBuilder toolbar and select an object. Edit the object as necessary: To translate, rotate, or scale the selected object, drag the transform handles that appear on it. To move ...",t:"Editing Objects"},"731":{i:0.00197470413386519,u:"../content/comp_environment/modelbuilder/setting_initial_action_center.html",a:"When you translate, rotate, or scale vertices, edges, faces, or objects, you can change the initial position from where the action originates (that is, where the transform handles appear whenever you change the selection). To do so: Make sure Edit Mode  is active in the ModelBuilder toolbar. To ...",t:"Setting the Initial Action Center for Translate, Rotate, and Scale"},"732":{i:0.000411712592762673,u:"../content/comp_environment/modelbuilder/applying_textures.html",a:"There are a couple of ways to texture your ModelBuilder models in Nuke: If your 3D model closely matches the original 2D footage, you can project the 2D footage onto the geometry. See  Projecting Textures onto Your Shapes . If you have added new objects over the top of your 2D footage (for example, ...",t:"Applying Textures"},"733":{i:0.000692764223869896,u:"../content/comp_environment/modelbuilder/projecting_textures.html",a:" If your ModelBuilder model closely matches the original 2D footage, you can simply project the 2D footage onto the geometry. To do so: At the bottom of the ModelBuilder properties panel, set the bake menu  to Projection and click Bake. This creates a projection at the current texture frame: a ...",t:"Projecting Textures onto Your Shapes"},"734":{i:0.000692764223869896,u:"../content/comp_environment/modelbuilder/uv_unwrapping.html",a:"If you\u0027ve used ModelBuilder to add  new objects over the top of your 2D footage (for example, added a new window to a building or extra bins alongside a street), you can\u0027t project your 2D footage over those objects because they were never in the original footage. Instead, you need to supply a ...",t:"UV Unwrapping"},"735":{i:0.00063489566611476,u:"../content/comp_environment/modelbuilder/exporting_separate_geo_nodes.html",a:"If necessary, you can export shapes on your ModelBuilder model to separate geometry nodes. This allows you to operate on one part of the scene separately from the rest. You can use the geometry nodes in the same way as any other geometry nodes in Nuke. To Export Shapes to Separate Geometry Nodes: In ...",t:"Exporting Shapes to Separate Geometry Nodes"},"736":{i:0.000376324095958171,u:"../content/comp_environment/particles/creating_3d_particles.html",a:"Nuke\u0027s Particle node set is a solution for creating particles in a 3D environment. You can create things like smoke, fog, falling snow, explosions, and bubbles - the possibilities are endless. You can use the various Particle nodes for emitting, manipulating, and displaying limitless types of ...",t:"Creating 3D Particles"},"737":{i:0.000374811347343722,u:"../content/comp_environment/particles/connecting_particle_nodes.html",a:"In order to create particles, the minimum setup you need is the ParticleEmitter node. To connect your Particle nodes:  Click the Particles menu in the Toolbar and select the ParticleEmitter node.  Connect it to a Viewer node.  Connect a 3D geometry object, or PositionToPoints point cloud, to the ...",t:"Connecting Particle Nodes"},"738":{i:0.000797252083188287,u:"../content/comp_environment/particles/emitting_particles.html",a:"The ParticleEmitter node is the first and only required node in a minimum particle setup. Once you’ve created a ParticleEmitter, connected it to a Viewer and clicked play on the timeline, you’ll see the default set of particles emitting (from a 3D geometry or point cloud, if you’ve connected one). ...",t:"Emitting Particles"},"739":{i:0.000389817126924729,u:"../content/comp_environment/particles/spawning_particles.html",a:"If you’re looking to have your existing particles emit even more particles, you should turn to ParticleSpawn.  Connect the ParticleSpawn node to your particle stream (the ParticleEmitter output, for example). All the particles emitted now start spawning more particles. Adjust the ParticleSpawn ...",t:"Spawning Particles with ParticleSpawn"},"740":{i:0.00125271290585765,u:"../content/comp_environment/particles/adjusting_speed_direction.html",a:"Applying Gravity to Particles When applying gravity to particles, as opposed to our familiar gravity, Nuke doesn’t restrict you to a certain direction but works in any or all of the x, y and z directions. You can add gravity either by:  Using the ParticleDirectionalForce to apply a directional ...",t:"Adjusting the Speed and Direction of the Particles"},"741":{i:0.00105800258134777,u:"../content/comp_environment/particles/modifying_particle_movement.html",a:"Bouncing Particles off Objects With the ParticleBounce node, you can make your particles bounce off the shape of a 3D object  instead of traveling through it. Connect this node to your other particle nodes, and adjust the ParticleBounce controls:  To set your particles to bounce off the outside of ...",t:"Modifying the Particles’ Movement"},"742":{i:0.000321498003524325,u:"../content/comp_environment/particles/adjusting_common_controls.html",a:"Many of the particle nodes share a number of controls, such as rendering and transform controls. The following covers the use of these.  Particle Rendering Controls The first controls on the properties panels of all the particle nodes have to do with how the particles are output to the Viewer and ...",t:"Adjusting Controls Common to Several Particle Nodes"},"743":{i:0.000458136250325134,u:"../content/comp_environment/particles/adjusting_particles_using_curves.html",a:"With the ParticleCurve, you can apply a curve to particle properties (such as size or mass) to change them over time.  Connect the node to your particle node stream. Adjust the curves in the ParticleCurve properties panel. The x axis represents the lifetime of the particles. r - adjust the curve for ...",t:"Adjusting Particle Properties Using Curves"},"744":{i:0.000796767381217536,u:"../content/comp_environment/particles/adjusting_particles_using_expressions.html",a:"With the ParticleExpression node, you can adjust your particles by setting expressions on their attributes. Using expressions gives you a vast variety of ways of adjusting how your particles behave. You can use a similar expression syntax as you would elsewhere in Nuke, with the exception that some ...",t:"Adjusting Particles Using Expressions"},"745":{i:0.000648087840945341,u:"../content/comp_environment/particles/adjusting_particle_settings.html",a:"If you want to adjust how many steps of particle simulation take place per animation frame, you can use the steps per frame control in the ParticleSettings properties panel. Sometimes simulations cannot generate enough accuracy only calculating once per frame, and the resulting particle movement can ...",t:"Adjusting Particle Simulation Settings"},"746":{i:0.000913368120647286,u:"../content/comp_environment/particles/merging_particle_streams.html",a:"If you have more than one set of particle nodes, and you want to combine them into one stream, ParticleMerge is your node. You can merge as many particle streams as you need into a single ParticleMerge.  Connect a ParticleMerge node to other particle nodes at any point in the particle stream.  ...",t:"Merging Particle Streams"},"747":{i:0.000594774497125944,u:"../content/comp_environment/particles/turning_particles_3d_geometry.html",a:"With the ParticleToGeo node, you can create 3D geometry for your particles. If you have a 3D geometry object connected anywhere in your particle stream, ParticleToGeo takes the particles and turn them into 3D objects for example for use in the Viewer or Scene node. ParticleToGeo make copies of the ...",t:"Turning Particles into 3D Geometry"},"748":{i:0.000648087840945341,u:"../content/comp_environment/particles/caching_particles.html",a:"The ParticleCache node allows you to store the geometry simulation for a particle system to file. It can then be read back in different sessions of Nuke or on different machines without the need for recalculation. This allows a particle system to be produced by an artist and then used by a render ...",t:"Caching Particles"},});